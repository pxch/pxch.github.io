[{"authors":["admin"],"categories":null,"content":"I am a research scientist and team lead in the AI group at Bloomberg, where I manage the Core NLP team, focusing on developing libraries and frameworks for NLP and LLM applications and training foundation models with financial domain knowledge.\nPrior to joining Bloomberg, I obtained my Ph.D. in Computer Science at UT Austin, working on natural language understanding and computational semantics with Dr. Katrin Erk. I completed my undergraduate studies at Tsinghua University, majoring in Automation and Economics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.pengxiang.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a research scientist and team lead in the AI group at Bloomberg, where I manage the Core NLP team, focusing on developing libraries and frameworks for NLP and LLM applications and training foundation models with financial domain knowledge.","tags":null,"title":"Pengxiang Cheng","type":"authors"},{"authors":["Ozan İrsoy","Pengxiang Cheng","Jennifer L Chen","Daniel Preoțiuc-Pietro","Shiyue Zhang","Duccio Pappadopulo"],"categories":[],"content":"","date":1762300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1762300800,"objectID":"3706d4ead4e1651f4df321be8642996f","permalink":"https://www.pengxiang.me/publication/emnlp2025/","publishdate":"2025-08-23T22:27:01-04:00","relpermalink":"/publication/emnlp2025/","section":"publication","summary":"Instruct models, obtained from various instruction tuning or post-training steps, are commonly deemed superior and more usable than their base counterpart. While the model gains instruction following ability, instruction tuning may lead to forgetting the knowledge from pre-training or it may encourage the model being overly conversational or verbose. This, in turn, can lead to degradation of in-context few-shot learning performance. In this work, we study the performance trajectory between base and instruct models by scaling down the strength of instruction-tuning via the partial adaption method. We show that, across several model families and model sizes, reducing the strength of instruction-tuning results in material improvement on a few-shot in-context learning benchmark covering a variety of classic natural language tasks. This comes at the cost of losing some degree of instruction following ability as measured by AlpacaEval. Our study shines light on the potential trade-off between in-context learning and instruction following abilities that is worth considering in practice.","tags":[],"title":"Improving Instruct Models for Free: A Study on Partial Adaptation","type":"publication"},{"authors":["Aldo Porco","Dhruv Mehra","Igor Malioutov","Karthik Radhakrishnan","Moniba Keymanesh","Daniel Preoțiuc-Pietro","Sean MacAvaney","Pengxiang Cheng"],"categories":[],"content":"","date":1752364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752364800,"objectID":"f59995560d4571aabd5b064fd34c1122","permalink":"https://www.pengxiang.me/publication/sigir2025/","publishdate":"2025-05-21T23:17:03-04:00","relpermalink":"/publication/sigir2025/","section":"publication","summary":"Learned Sparse Retrieval (LSR) models encode text as weighted term vectors, which need to be sparse to leverage inverted index structures during retrieval. SPLADE, the most popular LSR model, uses FLOPS regularization to encourage vector sparsity during training. However, FLOPS regularization does not ensure sparsity among terms - only within a given query or document. Terms with very high Document Frequencies (DFs) substantially increase latency in production retrieval engines, such as Apache Solr, due to their lengthy posting lists. To address the issue of high DFs, we present a new variant of FLOPS regularization: DF-FLOPS. This new regularization technique penalizes the usage of high-DF terms, thereby shortening posting lists and reducing retrieval latency. Unlike other inference-time sparsification methods, such as stopword removal, DF-FLOPS regularization allows for the selective inclusion of high-frequency terms in cases where the terms are truly salient. We find that DF-FLOPS successfully reduces the prevalence of high-DF terms and lowers retrieval latency (around 10x faster) in a production-grade engine while maintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and cross-domain (improved performance in 12 out of 13 tasks on which we tested). With retrieval latencies on par with BM25, this work provides an important step towards making LSR practical for deployment in production-grade search engines.","tags":[],"title":"An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc","type":"publication"},{"authors":["Niklas Stoehr","Pengxiang Cheng","Jing Wang","Daniel Preoțiuc-Pietro","Rajarshi Bhowmik"],"categories":[],"content":"","date":1710720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710720000,"objectID":"49b6876cb438b3bc5ab939abb557e3e4","permalink":"https://www.pengxiang.me/publication/eacl2024/","publishdate":"2023-10-07T20:58:01-04:00","relpermalink":"/publication/eacl2024/","section":"publication","summary":"Language models contain ranking-based knowledge and are powerful solvers of in-context ranking tasks. For instance, they may have parametric knowledge about the ordering of countries by size or may be able to rank product reviews by sentiment. We compare pairwise, pointwise and listwise prompting techniques to elicit a language model’s ranking knowledge. However, we find that even with careful calibration and constrained decoding, prompting-based techniques may not always be self-consistent in the rankings they produce. This motivates us to explore an alternative approach that is inspired by an unsupervised probing method called Contrast-Consistent Search (CCS). The idea is to train a probe guided by a logical constraint: a language model’s representation of a statement and its negation must be mapped to contrastive true-false poles consistently across multiple statements. We hypothesize that similar constraints apply to ranking tasks where all items are related via consistent, pairwise or listwise comparisons. To this end, we extend the binary CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking methods such as the Max-Margin Loss, Triplet Loss and an Ordinal Regression objective. Across different models and datasets, our results confirm that CCR probing performs better or, at least, on a par with prompting.","tags":[],"title":"Unsupervised Contrast-Consistent Ranking with Language Models","type":"publication"},{"authors":["Genta Winata","Lingjue Xie","Karthik Radhakrishnan","Shijie Wu","Xisen Jin","Pengxiang Cheng","Mayank Kulkarni","Daniel Preoțiuc-Pietro"],"categories":[],"content":"","date":1688947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688947200,"objectID":"a103aa6c68a21825b20db79d20dbf3f4","permalink":"https://www.pengxiang.me/publication/acl2023/","publishdate":"2023-05-25T23:45:49-04:00","relpermalink":"/publication/acl2023/","section":"publication","summary":"Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past. In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering both classification and sequence labeling tasks. We present LR ADJUST, a learning rate scheduling method that is simple, yet effective in preserving new information without strongly overwriting past knowledge. Furthermore, we show that this method is effective across multiple continual learning approaches. Finally, we provide further insights into the dynamics of catastrophic forgetting in this massively multilingual setup.","tags":[],"title":"Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning","type":"publication"},{"authors":["Xisen Jin","Xiang Ren","Daniel Preoțiuc-Pietro","Pengxiang Cheng"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"0f76f794746abfc920cdd2a52d61c59c","permalink":"https://www.pengxiang.me/publication/iclr2023/","publishdate":"2023-01-26T23:23:18-05:00","relpermalink":"/publication/iclr2023/","section":"publication","summary":"Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.","tags":[],"title":"Dataless Knowledge Fusion by Merging Weights of Language Models","type":"publication"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581206400,"objectID":"fa85bfc04071c77276417f356a6a2b6b","permalink":"https://www.pengxiang.me/publication/aaai2020/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/aaai2020/","section":"publication","summary":"Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.","tags":[],"title":"Attending to Entities for Better Text Understanding","type":"publication"},{"authors":["Pengxiang Cheng","Alex Tomkovich","Eric Holgate","Su Wang","Katrin Erk"],"categories":[],"content":"","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"3cffa4c7a0479a5d999f6046d4e1e83f","permalink":"https://www.pengxiang.me/publication/tac2019/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/tac2019/","section":"publication","summary":"","tags":[],"title":"The UTexas System for TAC 2019 SM-KBP Task 3: Hypothesis Detection with Graph Convolutional Networks","type":"publication"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1548720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548720000,"objectID":"6ed36643404de7e13317e403982f1596","permalink":"https://www.pengxiang.me/publication/aaai2019/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/aaai2019/","section":"publication","summary":"Implicit arguments, which cannot be detected solely through syntactic cues, make it harder to extract predicate-argument tuples. We present a new model for implicit argument prediction that draws on reading comprehension, casting the predicate-argument tuple with the missing argument as a query. We also draw on pointer networks and multi-hop computation. Our model shows good performance on an argument cloze task as well as on a nominal implicit argument prediction task.","tags":[],"title":"Implicit Argument Prediction as Reading Comprehension","type":"publication"},{"authors":["Pengxiang Cheng","Eric Holgate","Katrin Erk"],"categories":[],"content":"","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"38b22c2b30c2f696a59dedc4d9787638","permalink":"https://www.pengxiang.me/publication/tac2018/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/tac2018/","section":"publication","summary":"","tags":[],"title":"The UTexas System for TAC SM-KBP Task 3: Probabilistic Generation of Coherent Hypotheses","type":"publication"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1527897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527897600,"objectID":"728e6946d66259b55bb767520df436ac","permalink":"https://www.pengxiang.me/publication/naacl2018/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/naacl2018/","section":"publication","summary":"Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale. This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions. We show that our model has superior performance on both synthetic and natural data.","tags":[],"title":"Implicit Argument Prediction with Event Knowledge","type":"publication"},{"authors":["I. Beltagy","Stephen Roller","Pengxiang Cheng","Katrin Erk","Raymond J. Mooney"],"categories":[],"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"855fc05299756569264c5fe46ce4985b","permalink":"https://www.pengxiang.me/publication/cl2016/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/cl2016/","section":"publication","summary":"NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system: 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.","tags":[],"title":"Representing Meaning with a Combination of Logical and Distributional Models","type":"publication"}]