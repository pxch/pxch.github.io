[{"authors":["admin"],"categories":null,"content":"I am a senior research scientist at Bloomberg AI, working on entity-centric analysis on news, social media, and financial documents, including named entity recognition, entity linking, and entity salience prediction.\nI recently completed my Ph.D. in the Department of Computer Science at The University of Texas at Austin, advised by Professor Katrin Erk. During my time at UT Austin, my research interests lie in Natural Language Processing (NLP) and Computational Semantics. My dissertation focused on different approaches of integrating structural semantic knowledge into end-to-end neural models for better natural language understanding and reasoning.\nBefore coming to UT Austin, I completed my undergraduate studies at Tsinghua University, majoring in Automation and Economics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.pengxiang.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a senior research scientist at Bloomberg AI, working on entity-centric analysis on news, social media, and financial documents, including named entity recognition, entity linking, and entity salience prediction.","tags":null,"title":"Pengxiang Cheng","type":"authors"},{"authors":null,"categories":null,"content":"What is TACC? TACC Systems Useful Links ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"dae4c3d899fa11bbd807285801f77494","permalink":"https://www.pengxiang.me/tutorials/tacc-tips/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/tutorials/tacc-tips/","section":"tutorials","summary":"Some personal tips on how to best use the TACC resources for NLP experiments.","tags":null,"title":"TACC Overview","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5eaf7c221be85a1bff08d91779c0527c","permalink":"https://www.pengxiang.me/tutorials/tacc-tips/general/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/tutorials/tacc-tips/general/","section":"tutorials","summary":"","tags":null,"title":"General Tips","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"8833da55709ecc8bb6d2428fc7a518c9","permalink":"https://www.pengxiang.me/tutorials/tacc-tips/maverick2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/tutorials/tacc-tips/maverick2/","section":"tutorials","summary":"","tags":null,"title":"Maverick2","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"a755e2d116f3ceb35c373b4fb52995bd","permalink":"https://www.pengxiang.me/tutorials/tacc-tips/chameleon/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/tutorials/tacc-tips/chameleon/","section":"tutorials","summary":"","tags":null,"title":"Chameleon","type":"docs"},{"authors":null,"categories":null,"content":"Last updated on 04/15/2020. Also available in PDF format. Education   University of Texas at Austin, Austin, TX\nPh.D. candidate in Computer Science, expected June 2020\nThesis: Learning Better Latent Representations from Semantic Knowledge\nAdvisor: Katrin Erk\n Tsinghua University, Beijing, China\nB.Eng. in Automation and B.Econ. in Economics, 2013\nPublications    Pengxiang Cheng, Katrin Erk. 2020. Attending to Entities for Better Text Understanding. Proceedings of AAAI Conference on Artificial Intelligence (AAAI).\n  Pengxiang Cheng, Alex Tomkovich, Eric Holgate, Su Wang, and Katrin Erk. 2019. The UTexas System for TAC 2019 SM-KBP Task 3: Hypothesis Detection with Graph Convolutional Networks. Proceedings of Text Analysis Conference (TAC).\n  Pengxiang Cheng, Katrin Erk. 2019. Implicit Argument Prediction as Reading Comprehension. Proceedings of AAAI Conference on Artificial Intelligence (AAAI).\n  Pengxiang Cheng, Eric Holgate, Katrin Erk. 2018. The UTexas System for TAC SM-KBP Task 3: Probabilistic Generation of Coherent Hypotheses. Proceedings of Text Anal- ysis Conference (TAC).\n  Pengxiang Cheng, Katrin Erk. 2018. Implicit Argument Prediction with Event Knowledge. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL).\n  I. Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, Raymond Mooney. 2016. Representing Meaning with a Combination of Logical and Distributional Models. Computational Linguistics (CL), 42(4).\n  Yalin Sun, Pengxiang Cheng, Shengwei Wang, Hao Lyu, Matthew Lease, Iain Marshal, Byron C. Wallace. 2016. Crowdsourcing Information Extraction for Biomedical Systematic Reviews. 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP): Works-in-Progress Track.\n  Research Experience  Industry Experience  Teaching Experience  Service  Awards  Skills  ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://www.pengxiang.me/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"Last updated on 04/15/2020. Also available in PDF format. Education   University of Texas at Austin, Austin, TX\nPh.D. candidate in Computer Science, expected June 2020\nThesis: Learning Better Latent Representations from Semantic Knowledge","tags":null,"title":"CV","type":"page"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1581206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581206400,"objectID":"fa85bfc04071c77276417f356a6a2b6b","permalink":"https://www.pengxiang.me/publication/aaai2020/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/aaai2020/","section":"publication","summary":"Recent progress in NLP witnessed the development of large-scale pre-trained language models (GPT, BERT, XLNet, etc.) based on Transformer (Vaswani et al. 2017), and in a range of end tasks, such models have achieved state-of-the-art results, approaching human performance. This demonstrates the power of the stacked self-attention architecture when paired with a sufficient number of layers and a large amount of pre-training data. However, on tasks that require complex and long-distance reasoning where surface-level cues are not enough, there is still a large gap between the pre-trained models and human performance. Strubell et al. (2018) recently showed that it is possible to inject knowledge of syntactic structure into a model through supervised self-attention. We conjecture that a similar injection of semantic knowledge, in particular, coreference information, into an existing model would improve performance on such complex problems. On the LAMBADA (Paperno et al. 2016) task, we show that a model trained from scratch with coreference as auxiliary supervision for self-attention outperforms the largest GPT-2 model, setting the new state-of-the-art, while only containing a tiny fraction of parameters compared to GPT-2. We also conduct a thorough analysis of different variants of model architectures and supervision configurations, suggesting future directions on applying similar techniques to other problems.","tags":[],"title":"Attending to Entities for Better Text Understanding","type":"publication"},{"authors":["Pengxiang Cheng","Alex Tomkovich","Eric Holgate","Su Wang","Katrin Erk"],"categories":[],"content":"","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"3cffa4c7a0479a5d999f6046d4e1e83f","permalink":"https://www.pengxiang.me/publication/tac2019/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/tac2019/","section":"publication","summary":"","tags":[],"title":"The UTexas System for TAC 2019 SM-KBP Task 3: Hypothesis Detection with Graph Convolutional Networks","type":"publication"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1548720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548720000,"objectID":"6ed36643404de7e13317e403982f1596","permalink":"https://www.pengxiang.me/publication/aaai2019/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/aaai2019/","section":"publication","summary":"Implicit arguments, which cannot be detected solely through syntactic cues, make it harder to extract predicate-argument tuples. We present a new model for implicit argument prediction that draws on reading comprehension, casting the predicate-argument tuple with the missing argument as a query. We also draw on pointer networks and multi-hop computation. Our model shows good performance on an argument cloze task as well as on a nominal implicit argument prediction task.","tags":[],"title":"Implicit Argument Prediction as Reading Comprehension","type":"publication"},{"authors":["Pengxiang Cheng","Eric Holgate","Katrin Erk"],"categories":[],"content":"","date":1542067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542067200,"objectID":"38b22c2b30c2f696a59dedc4d9787638","permalink":"https://www.pengxiang.me/publication/tac2018/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/tac2018/","section":"publication","summary":"","tags":[],"title":"The UTexas System for TAC SM-KBP Task 3: Probabilistic Generation of Coherent Hypotheses","type":"publication"},{"authors":["Pengxiang Cheng","Katrin Erk"],"categories":[],"content":"","date":1527897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527897600,"objectID":"728e6946d66259b55bb767520df436ac","permalink":"https://www.pengxiang.me/publication/naacl2018/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/naacl2018/","section":"publication","summary":"Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale. This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions. We show that our model has superior performance on both synthetic and natural data.","tags":[],"title":"Implicit Argument Prediction with Event Knowledge","type":"publication"},{"authors":["I. Beltagy","Stephen Roller","Pengxiang Cheng","Katrin Erk","Raymond J. Mooney"],"categories":[],"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"855fc05299756569264c5fe46ce4985b","permalink":"https://www.pengxiang.me/publication/cl2016/","publishdate":"2020-04-01T00:00:00-05:00","relpermalink":"/publication/cl2016/","section":"publication","summary":"NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.We adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system: 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.","tags":[],"title":"Representing Meaning with a Combination of Logical and Distributional Models","type":"publication"}]