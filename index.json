[
  {
    "authors": [
      "**Pengxiang Cheng**",
      "Katrin Erk"
    ],
    "categories": null,
    "content": "",
    "date": 1548741600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1548741600,
    "objectID": "6ed36643404de7e13317e403982f1596",
    "permalink": "http://pengxiang.me/publication/aaai2019/",
    "publishdate": "2019-01-29T00:00:00-06:00",
    "relpermalink": "/publication/aaai2019/",
    "section": "publication",
    "summary": "Implicit arguments, which cannot be detected solely through syntactic cues, make it harder to extract predicate-argument tuples. We present a new model for implicit argument prediction that draws on reading comprehension, casting the predicate-argument tuple with the missing argument as a query. We also draw on pointer networks and multi-hop computation. Our model shows good performance on an argument cloze task as well as on a nominal implicit argument prediction task.",
    "tags": [],
    "title": "Implicit Argument Prediction as Reading Comprehension",
    "type": "publication"
  },
  {
    "authors": [
      "**Pengxiang Cheng**",
      "Katrin Erk"
    ],
    "categories": null,
    "content": "",
    "date": 1527915600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1527915600,
    "objectID": "728e6946d66259b55bb767520df436ac",
    "permalink": "http://pengxiang.me/publication/naacl2018/",
    "publishdate": "2018-06-02T00:00:00-05:00",
    "relpermalink": "/publication/naacl2018/",
    "section": "publication",
    "summary": "Implicit arguments are not syntactically connected to their predicates, and are therefore hard to extract. Previous work has used models with large numbers of features, evaluated on very small datasets. We propose to train models for implicit argument prediction on a simple cloze task, for which data can be generated automatically at scale. This allows us to use a neural model, which draws on narrative coherence and entity salience for predictions. We show that our model has superior performance on both synthetic and natural data.",
    "tags": [],
    "title": "Implicit Argument Prediction with Event Knowledge",
    "type": "publication"
  },
  {
    "authors": [
      "I. Beltagy",
      "Stephen Roller",
      "**Pengxiang Cheng**",
      "Katrin Erk",
      "Raymond J. Mooney"
    ],
    "categories": null,
    "content": "",
    "date": 1480572000,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1480572000,
    "objectID": "855fc05299756569264c5fe46ce4985b",
    "permalink": "http://pengxiang.me/publication/cl2016/",
    "publishdate": "2016-12-01T00:00:00-06:00",
    "relpermalink": "/publication/cl2016/",
    "section": "publication",
    "summary": "NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements. Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning. Distributional models give graded similarity ratings for words and phrases, but do not capture sentence structure in the same detail as logic-based approaches. It has therefore been argued that the two are complementary.\nWe adopt a hybrid approach that combines logical and distributional semantics using probabilistic logic, specifically Markov Logic Networks. In this article, we focus on the three components of a practical system: 1) Logical representation focuses on representing the input problems in probabilistic logic; 2) knowledge base construction creates weighted inference rules by integrating distributional information with other sources; and 3) probabilistic inference involves solving the resulting MLN inference problems efficiently. To evaluate our approach, we use the task of textual entailment, which can utilize the strengths of both logic-based and distributional representations. In particular we focus on the SICK data set, where we achieve state-of-the-art results. We also release a lexical entailment data set of 10,213 rules extracted from the SICK data set, which is a valuable resource for evaluating lexical entailment systems.",
    "tags": [],
    "title": "Representing Meaning with a Combination of Logical and Distributional Models",
    "type": "publication"
  }
]