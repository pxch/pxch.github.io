@inproceedings{irsoy-etal-2025-improving,
    title = "Improving Instruct Models for Free: A Study on Partial Adaptation",
    author = "Irsoy, Ozan  and
      Cheng, Pengxiang  and
      Chen, Jennifer L  and
      Preotiuc-Pietro, Daniel  and
      Zhang, Shiyue  and
      Pappadopulo, Duccio",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.581/",
    doi = "10.18653/v1/2025.emnlp-main.581",
    pages = "11507--11521",
    ISBN = "979-8-89176-332-6",
    abstract = "Instruct models, obtained from various instruction tuning or post-training steps, are commonly deemed superior and more usable than their base counterpart. While the model gains instruction following ability, instruction tun- ing may lead to forgetting the knowledge from pre-training or it may encourage the model being overly conversational or verbose. This, in turn, can lead to degradation of in-context few-shot learning performance. In this work, we study the performance trajectory between base and instruct models by scaling down the strength of instruction-tuning via the partial adaption method. We show that, across several model families and model sizes, reducing the strength of instruction-tuning results in material improvement on a few-shot in-context learning benchmark covering a variety of classic natural language tasks. This comes at the cost of losing some degree of instruction following ability as measured by AlpacaEval. Our study shines light on the potential trade-off between in-context learning and instruction following abilities that is worth considering in practice."
}
